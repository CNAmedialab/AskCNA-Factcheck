"""
äº‹å¯¦æŸ¥æ ¸å°è©±æ©Ÿå™¨äºº - Streamlitç‰ˆæœ¬
ç›´æ¥èª¿ç”¨ç¾æœ‰çš„functionså’Œagenticæ¨¡çµ„
"""
import streamlit as st
import asyncio
from functions import get_check_points, es_resources
from agentic import (
    generate_explanation,
    generate_explanation_streaming,
    run_question_review,
    final_report_agent,
    final_report_agent_streaming
)
from datetime import datetime

def run_async_sync(coroutine):
    """åŒæ­¥åŸ·è¡Œç•°æ­¥å‡½æ•¸çš„è¼”åŠ©å‡½æ•¸"""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # å¦‚æœæœ‰æ´»å‹•çš„äº‹ä»¶å¾ªç’°ï¼Œå‰µå»ºä¸€å€‹æ–°çš„
            import threading
            import concurrent.futures

            def run_in_thread():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(coroutine)
                finally:
                    new_loop.close()

            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_in_thread)
                return future.result()
        else:
            return loop.run_until_complete(coroutine)
    except RuntimeError:
        # æ²’æœ‰äº‹ä»¶å¾ªç’°ï¼Œå‰µå»ºä¸€å€‹æ–°çš„
        return asyncio.run(coroutine)

def create_streaming_generator_with_result(async_streaming_func, *args, **kwargs):
    """å‰µå»ºåŒæ­¥çš„ generator ä¾†åŒ…è£ç•°æ­¥ streaming å‡½æ•¸ï¼Œä¸¦è¿”å›å®Œæ•´æ–‡æœ¬"""
    import threading
    import queue
    import concurrent.futures

    result_queue = queue.Queue()
    full_text_ref = {"text": ""}  # ä½¿ç”¨å­—å…¸ä¾†ä¿æŒå¼•ç”¨

    def run_async_streaming():
        new_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(new_loop)
        try:
            async def collect_chunks():
                async for chunk in async_streaming_func(*args, **kwargs):
                    full_text_ref["text"] += chunk
                    result_queue.put(chunk)
                result_queue.put(None)  # çµæŸä¿¡è™Ÿ

            new_loop.run_until_complete(collect_chunks())
        finally:
            new_loop.close()

    # åœ¨èƒŒæ™¯ç·šç¨‹ä¸­é‹è¡Œç•°æ­¥å‡½æ•¸
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(run_async_streaming)

        def generator():
            while True:
                try:
                    chunk = result_queue.get(timeout=30)  # 30ç§’è¶…æ™‚
                    if chunk is None:  # çµæŸä¿¡è™Ÿ
                        break
                    yield chunk
                except queue.Empty:
                    break

            # ç­‰å¾…ç·šç¨‹å®Œæˆ
            future.result()

        return generator(), full_text_ref

def create_streaming_generator(async_streaming_func, *args, **kwargs):
    """å‰µå»ºåŒæ­¥çš„ generator ä¾†åŒ…è£ç•°æ­¥ streaming å‡½æ•¸"""
    generator, _ = create_streaming_generator_with_result(async_streaming_func, *args, **kwargs)
    return generator

class StreamlitFactCheckBot:
    def __init__(self):
        pass

    def reset_fact_check_state(self):
        """é‡ç½®äº‹å¯¦æŸ¥æ ¸ç‹€æ…‹"""
        st.session_state.fact_check_state = None
        st.session_state.current_draft = None
        st.session_state.check_points = None
        st.session_state.resources = None
        st.session_state.user_input = None
        st.session_state.round_num = 1
        st.session_state.history = []
        st.session_state.messages = []
        st.session_state.ai_suggested_question = None


    def start_fact_check(self, user_input: str, media_name: str = "Chiming"):
        """é–‹å§‹äº‹å¯¦æŸ¥æ ¸æµç¨‹"""
        # é‡ç½®ç‹€æ…‹ï¼ˆä½†ä¿ç•™messagesï¼‰
        st.session_state.fact_check_state = "starting"
        st.session_state.current_draft = None
        st.session_state.check_points = None
        st.session_state.resources = None
        st.session_state.user_input = user_input
        st.session_state.round_num = 1
        st.session_state.history = []
        st.session_state.ai_suggested_question = None

        print(f"====> é–‹å§‹å°è©±ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # æ­¥é©Ÿ1: åˆ†ææŸ¥æ ¸é»
        with st.spinner("ğŸ” æ­£åœ¨åˆ†ææŸ¥æ ¸é»..."):
            check_points_data = get_check_points(user_input, media_name)
            if check_points_data["Result"] == "Y":
                check_points = check_points_data["ResultData"]["check_points"]
                st.session_state.check_points = check_points
                with st.chat_message("assistant"):
                    st.markdown("**æŸ¥æ ¸é»åˆ†æå®Œæˆï¼š**")
                    if isinstance(check_points, list):
                        # æª¢æŸ¥æŸ¥æ ¸é»æ˜¯å¦å·²ç¶“æœ‰ç·¨è™Ÿï¼Œå¦‚æœæœ‰å°±ç›´æ¥é¡¯ç¤ºï¼Œæ²’æœ‰å°±åŠ ä¸Šç·¨è™Ÿ
                        formatted_points = []
                        for i, cp in enumerate(check_points):
                            cp_str = str(cp).strip()
                            # æª¢æŸ¥æ˜¯å¦å·²ç¶“ä»¥æ•¸å­—é–‹é ­ï¼ˆå·²æœ‰ç·¨è™Ÿï¼‰
                            if cp_str and cp_str[0].isdigit() and '. ' in cp_str[:5]:
                                formatted_points.append(cp_str)
                            else:
                                formatted_points.append(f"{i+1}. {cp_str}")
                        check_points_text = "\n".join(formatted_points)
                        st.markdown(f"\n{check_points_text}")
                    else:
                        check_points_text = f"{check_points}"
                        st.markdown(f"\n{check_points_text}")

                st.session_state.messages.append({
                    "role": "assistant",
                    "content": f"**æŸ¥æ ¸é»åˆ†æå®Œæˆ:**\n\n{check_points_text}"
                })
            else:
                check_points = None
                st.session_state.check_points = None
                with st.chat_message("assistant"):
                    st.warning("âš ï¸ æŸ¥æ ¸é» API å¤±æ•—ï¼Œå°‡ä½¿ç”¨åŸå§‹æ–‡æœ¬é€²è¡ŒæŸ¥æ ¸")

                # è¨˜éŒ„æŸ¥æ ¸é»å¤±æ•—è¨Šæ¯
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": "âš ï¸ æŸ¥æ ¸é» API å¤±æ•—ï¼Œå°‡ä½¿ç”¨åŸå§‹æ–‡æœ¬é€²è¡ŒæŸ¥æ ¸"
                })

        # æ­¥é©Ÿ2: æœç´¢ç›¸é—œè³‡æº
        with st.spinner("ğŸ“š æ­£åœ¨æœç´¢ç›¸é—œè­‰æ“šè³‡æ–™..."):
            resources = es_resources(user_input)
            st.session_state.resources = resources
            with st.chat_message("assistant"):
                st.markdown(f"**ä¸€å…±æ‰¾åˆ°{len(resources)}ç­†è­‰æ“šè³‡æ–™**")

        # è¨˜éŒ„è­‰æ“šæœç´¢çµæœ
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"ä¸€å…±æ‰¾åˆ°{len(resources)}ç­†è­‰æ“šè³‡æ–™"
        })

        # æ­¥é©Ÿ3: ç”Ÿæˆåˆæ­¥æŸ¥æ ¸çµæœ
        with st.chat_message("assistant"):
            with st.spinner("ğŸ’¡ æ­£åœ¨ç”Ÿæˆåˆæ­¥æŸ¥æ ¸çµæœ..."):
                def explanation_generator():
                    return create_streaming_generator(
                        generate_explanation_streaming,
                        user_input, check_points, resources, ""
                    )

                st.markdown("**åˆæ­¥æŸ¥æ ¸çµæœ**")
                draft = st.write_stream(explanation_generator())

        st.session_state.current_draft = draft

        # è¨˜éŒ„æŸ¥æ ¸è§£é‡‹çµæœ
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"{draft}"
        })

        # æ­¥é©Ÿ4: AIè©•ä¼°
        # é å…ˆé¡¯ç¤ºAIè©•ä¼°é–‹å§‹è¨Šæ¯
        with st.chat_message("assistant"):
            with st.spinner("ğŸ‘ï¸â€ğŸ—¨ï¸ æ­£åœ¨è©•ä¼°æŸ¥æ ¸çµæœ..."):
                eval_result = run_async_sync(run_question_review(draft, check_points))

                eval_result_content = f"""ğŸ“Š **AIè©•ä¼°çµæœï¼š**\n\n
â€¢  èªªæœåŠ›: {eval_result.persuasiveness}/5\n
â€¢  é‚è¼¯æ­£ç¢ºæ€§: {eval_result.logical_correctness}/5\n
â€¢  å®Œæ•´æ€§: {eval_result.completeness}/5\n
â€¢  ç°¡æ½”æ€§: {eval_result.conciseness}/5\n
â€¢  èªåŒåº¦: {eval_result.agreement}/5\n
â€¢  **å¹³å‡åˆ†æ•¸: {eval_result.average}/5**\n
â€¢  **æœ€å¼±é¢å‘**: {eval_result.weakest_aspect}"""
                st.markdown(eval_result_content)

        # è¨˜éŒ„AIè©•ä¼°çµæœ
        st.session_state.messages.append({
            "role": "assistant",
            "content": eval_result_content
        })

        # ç¬¬äºŒå€‹è¨Šæ¯ï¼šAIå»ºè­°çš„å•é¡Œ
        with st.chat_message("assistant"):
            ai_question_content = f"""â“ **AIå»ºè­°çš„æ”¹é€²å•é¡Œï¼š**
{eval_result.improvement_question}"""
            st.markdown(ai_question_content)

        # è¨˜éŒ„AIå»ºè­°å•é¡Œ
        st.session_state.messages.append({
            "role": "assistant",
            "content": ai_question_content
        })

        # ç¬¬ä¸‰å€‹è¨Šæ¯ï¼šé¸æ“‡ä¸‹ä¸€æ­¥å‹•ä½œ
        with st.chat_message("assistant"):
            next_step_content = """**è«‹é¸æ“‡ä¸‹ä¸€æ­¥å‹•ä½œï¼š**"""
            st.markdown(next_step_content)

        # è¨˜éŒ„é¸æ“‡é¸é …
        st.session_state.messages.append({
            "role": "assistant",
            "content": next_step_content
        })

        # è¨˜éŒ„è©•ä¼°æ­·å²
        st.session_state.history.append({
            "round": st.session_state.round_num,
            "explanation": draft,
            "evaluation": eval_result,
            "question": "",
            "ai_evaluation": eval_result,
            "user_question": eval_result.improvement_question,
            "question_source": "AIè‡ªå‹•å»ºè­°"
        })
        st.session_state.round_num += 1

        st.session_state.fact_check_state = "waiting_user_choice"
        st.session_state.ai_suggested_question = eval_result.improvement_question

    def handle_user_choice(self, choice: str):
        """è™•ç†ç”¨æˆ¶é¸æ“‡"""
        choice = choice.strip()

        if choice == "1":
            # æ¡ç”¨AIå»ºè­°çš„å•é¡Œ
            question = st.session_state.ai_suggested_question
            self.continue_with_question(question, "AIå»ºè­°")

        elif choice == "2":
            # è‡ªè¡Œè¼¸å…¥å•é¡Œ
            with st.chat_message("assistant"):
                st.markdown("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š")
            st.session_state.messages.append({"role": "assistant", "content": "è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š"})
            st.session_state.fact_check_state = "waiting_custom_question"

        elif choice == "3":
            # ç”Ÿæˆæœ€çµ‚å ±å‘Š
            self.generate_final_report()

    def apply_improvement(self, improvement_question: str):
        """æ‡‰ç”¨æ”¹å–„å•é¡Œä¸¦é‡æ–°ç”Ÿæˆ"""
        if st.session_state.fact_check_state != "waiting_for_improvement_choice":
            return

        # æ ¹æ“šæ”¹å–„å•é¡Œé‡æ–°ç”ŸæˆæŸ¥æ ¸çµæœ
        with st.chat_message("assistant"):
            st.markdown(f"âœ¨ **ç¬¬{st.session_state.round_num+1}è¼ªæ”¹å–„çµæœ**")

            def improvement_generator():
                return create_streaming_generator(
                    generate_explanation_streaming,
                    st.session_state.user_input,
                    st.session_state.check_points,
                    st.session_state.resources,
                    improvement_question
                )

            st.markdown(f"**ç¬¬{st.session_state.round_num-1}è¼ªå°è©±çµæœ**")
            draft = st.write_stream(improvement_generator())

        st.session_state.current_draft = draft
        st.session_state.round_num += 1

        # å°‡æ”¹å–„çµæœè¨˜éŒ„åˆ°messages
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"**ç¬¬{st.session_state.round_num-1}è¼ªå°è©±çµæœ**\n\n{draft}"
        })

        # æª¢æŸ¥æ˜¯å¦å·²é”æœ€å¤§è¼ªæ•¸
        if st.session_state.round_num > 3:
            st.session_state.fact_check_state = "ready_for_final_report"
            with st.chat_message("assistant"):
                st.markdown("â±ï¸ å·²é”æå•ä¸Šé™æ¬¡æ•¸ï¼Œç”Ÿæˆæœ€çµ‚å ±å‘Š") 
            st.session_state.messages.append({
                "role": "assistant",
                "content": "â±ï¸ å·²é”æå•ä¸Šé™æ¬¡æ•¸ï¼Œç”Ÿæˆæœ€çµ‚å ±å‘Š"
            })
        else:
            # è‡ªå‹•åŸ·è¡ŒAIè©•ä¼°
            eval_result = run_async_sync(run_question_review(draft, st.session_state.check_points))

            # é¡¯ç¤ºAIè©•ä¼°çµæœ
            with st.chat_message("assistant"):
                st.markdown(f"ğŸ” **ç¬¬{st.session_state.round_num}è¼ª AIè©•ä¼°çµæœ**")
                st.markdown(f"**æ•´é«”è©•åˆ†:** {eval_result.average}/5")
                st.markdown(f"**æœ€å¼±é¢å‘:** {eval_result.weakest_aspect}")
                st.markdown(f"**æ”¹å–„å•é¡Œ:** {eval_result.improvement_question}")

            # å°‡AIè©•ä¼°çµæœè¨˜éŒ„åˆ°messages
            st.session_state.messages.append({
                "role": "assistant",
                "content": f"ğŸ” **ç¬¬{st.session_state.round_num}è¼ª AIè©•ä¼°çµæœ**\n\n**æ•´é«”è©•åˆ†:** {eval_result.average}/5\n**æœ€å¼±é¢å‘:** {eval_result.weakest_aspect}\n**æ”¹å–„å•é¡Œ:** {eval_result.improvement_question}"
            })

            # è¨˜éŒ„è©•ä¼°æ­·å²
            st.session_state.history.append({
                "round": st.session_state.round_num,
                "ai_evaluation": eval_result,
                "user_question": eval_result.improvement_question,
                "question_source": "AIè‡ªå‹•å»ºè­°"
            })

            # æª¢æŸ¥æ˜¯å¦å“è³ªå·²é”æ¨™
            if eval_result.average >= 4.0:
                with st.chat_message("assistant"):
                    st.markdown(f"ğŸ¯ **å“è³ªé”æ¨™** - ç¬¬{st.session_state.round_num}è¼ªè©•ä¼°åˆ†æ•¸å·²é”{eval_result.average}/5ï¼Œå“è³ªè‰¯å¥½")
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": f"ğŸ¯ **å“è³ªé”æ¨™** - ç¬¬{st.session_state.round_num}è¼ªè©•ä¼°åˆ†æ•¸å·²é”{eval_result.average}/5ï¼Œå“è³ªè‰¯å¥½"
                })
                st.session_state.fact_check_state = "ready_for_final_report"
            else:
                # åˆ†æ•¸ä½æ–¼4ï¼Œé¡¯ç¤ºuserè¨Šæ¯å¼•å°é¸æ“‡
                with st.chat_message("user"):
                    st.markdown("è«‹é¸æ“‡ä¸‹ä¸€æ­¥ï¼šæ¡ç”¨AIå»ºè­°çš„æ”¹å–„å•é¡Œã€è‡ªè¡Œè¼¸å…¥æ”¹å–„å•é¡Œã€æˆ–ç›´æ¥ç”Ÿæˆæœ€çµ‚å ±å‘Šï¼Ÿ")

                st.session_state.messages.append({
                    "role": "user",
                    "content": "è«‹é¸æ“‡ä¸‹ä¸€æ­¥ï¼šæ¡ç”¨AIå»ºè­°çš„æ”¹å–„å•é¡Œã€è‡ªè¡Œè¼¸å…¥æ”¹å–„å•é¡Œã€æˆ–ç›´æ¥ç”Ÿæˆæœ€çµ‚å ±å‘Šï¼Ÿ"
                })

                st.session_state.fact_check_state = "waiting_for_improvement_choice"
                st.session_state.ai_suggested_question = eval_result.improvement_question

    def continue_with_question(self, question: str, source: str):
        """ç¹¼çºŒæŸ¥æ ¸æµç¨‹è™•ç†å•é¡Œ"""
        with st.chat_message("assistant"):
            with st.spinner("ğŸ’¡ æ­£åœ¨é‡æ–°ç”ŸæˆæŸ¥æ ¸è§£é‡‹..."):
                def regenerate_explanation_generator():
                    return create_streaming_generator(
                        generate_explanation_streaming,
                        st.session_state.user_input,
                        st.session_state.check_points,
                        st.session_state.resources,
                        question
                    )
                st.markdown(f"**ç¬¬{st.session_state.round_num-1}è¼ªå°è©±çµæœ**")
                draft = st.write_stream(regenerate_explanation_generator())

        # è¨˜éŒ„é‡æ–°ç”Ÿæˆçš„æŸ¥æ ¸è§£é‡‹
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"**ç¬¬{st.session_state.round_num-1}è¼ªå°è©±çµæœ**\n\n{draft}"
        })

        st.session_state.current_draft = draft

        # æª¢æŸ¥æ˜¯å¦å·²é”æœ€å¤§è¼ªæ•¸
        if st.session_state.round_num >= 3:
            with st.chat_message("assistant"):
                st.markdown("â±ï¸ **å·²é”æœ€å¤§æ”¹å–„è¼ªæ•¸**ï¼Œæº–å‚™ç”Ÿæˆæœ€çµ‚å ±å‘Š")

            # è¨˜éŒ„æœ€å¤§è¼ªæ•¸è¨Šæ¯
            st.session_state.messages.append({
                "role": "assistant",
                "content": "â±ï¸ **å·²é”æœ€å¤§æ”¹å–„è¼ªæ•¸**ï¼Œæº–å‚™ç”Ÿæˆæœ€çµ‚å ±å‘Š"
            })

            self.generate_final_report()
        else:
            # ç¹¼çºŒ AI è©•ä¼°
            # é å…ˆé¡¯ç¤ºAIè©•ä¼°é–‹å§‹è¨Šæ¯
            with st.chat_message("assistant"):
                st.markdown("ğŸ‘ï¸â€ğŸ—¨ï¸ æ­£åœ¨è©•ä¼°æŸ¥æ ¸çµæœ...")

            # è¨˜éŒ„AIè©•ä¼°é–‹å§‹è¨Šæ¯
            st.session_state.messages.append({
                "role": "assistant",
                "content": "AIè©•ä¼°æŸ¥æ ¸çµæœ..."
            })

            with st.spinner("è©•ä¼°ä¸­..."):
                eval_result = run_async_sync(run_question_review(draft, st.session_state.check_points))

            # ç¬¬ä¸€å€‹è¨Šæ¯ï¼šAIè©•ä¼°çµæœ
            with st.chat_message("assistant"):
                eval_result_content = f"""ğŸ“Š **AIè©•ä¼°çµæœï¼š**
â€¢  èªªæœåŠ›: {eval_result.persuasiveness}/5\n
â€¢  é‚è¼¯æ­£ç¢ºæ€§: {eval_result.logical_correctness}/5\n
â€¢  å®Œæ•´æ€§: {eval_result.completeness}/5\n
â€¢  ç°¡æ½”æ€§: {eval_result.conciseness}/5\n
â€¢  èªåŒåº¦: {eval_result.agreement}/5\n
â€¢  **å¹³å‡åˆ†æ•¸: {eval_result.average}/5**\n
â€¢  **æœ€å¼±é¢å‘**: {eval_result.weakest_aspect}"""
                st.markdown(eval_result_content)

            # è¨˜éŒ„AIè©•ä¼°çµæœ
            st.session_state.messages.append({
                "role": "assistant",
                "content": eval_result_content
            })

            # ç¬¬äºŒå€‹è¨Šæ¯ï¼šAIå»ºè­°çš„å•é¡Œ
            with st.chat_message("assistant"):
                ai_question_content = f"""â“ **AIå»ºè­°çš„æ”¹é€²å•é¡Œï¼š**
{eval_result.improvement_question}"""
                st.markdown(ai_question_content)

            # è¨˜éŒ„AIå»ºè­°å•é¡Œ
            st.session_state.messages.append({
                "role": "assistant",
                "content": ai_question_content
            })

            # ç¬¬ä¸‰å€‹è¨Šæ¯ï¼šé¸æ“‡ä¸‹ä¸€æ­¥å‹•ä½œ
            with st.chat_message("assistant"):
                next_step_content = """**è«‹é¸æ“‡ä¸‹ä¸€æ­¥å‹•ä½œï¼š**"""
                st.markdown(next_step_content)

            # è¨˜éŒ„é¸æ“‡é¸é …
            st.session_state.messages.append({
                "role": "assistant",
                "content": next_step_content
            })

            # è¨˜éŒ„è©•ä¼°æ­·å²
            st.session_state.history.append({
                "round": st.session_state.round_num,
                "explanation": draft,
                "evaluation": eval_result,
                "question": question,
                "question_source": source
            })
            st.session_state.round_num += 1

            st.session_state.fact_check_state = "waiting_user_choice"
            st.session_state.ai_suggested_question = eval_result.improvement_question

    def generate_final_report(self):
        """ç”Ÿæˆæœ€çµ‚å ±å‘Š"""
        # æ§‹å»ºå®Œæ•´æ­·å²
        full_history = f"åˆæ­¥è§£é‡‹: {st.session_state.current_draft}\n\n"
        for interaction in st.session_state.history:
            full_history += f"=== è¼ªæ¬¡{interaction['round']} ===\n"
            full_history += f"è§£é‡‹: {interaction.get('explanation', '')}\n"
            if interaction.get('question'):
                full_history += f"æå•: {interaction['question']}\n"
            full_history += f"AIè©•åˆ†: {interaction['evaluation'].average}/5 (æœ€å¼±é¢å‘: {interaction['evaluation'].weakest_aspect})\n\n"

        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        with st.chat_message("assistant"):
            with st.spinner("ğŸ“‹ æ­£åœ¨ç”Ÿæˆæœ€çµ‚å ±å‘Š..."):
                generator, full_text_ref = create_streaming_generator_with_result(
                    final_report_agent_streaming,
                    full_history,
                    st.session_state.check_points,
                    st.session_state.user_input,
                    st.session_state.resources
                )

            # é¡¯ç¤ºæ¨™é¡Œå’Œ streaming æ•ˆæœ
            st.markdown("**æœ€çµ‚æŸ¥æ ¸å ±å‘Š**")
            st.write_stream(generator)

            # ç²å–å®Œæ•´æ–‡æœ¬
            final_report = full_text_ref["text"]

        # è™•ç† Markdown æ ¼å¼ - è½‰ç¾©åƒè€ƒè³‡æ–™ç·¨è™Ÿä»¥é¿å…è§£æå•é¡Œ
        def escape_references(text):
            import re
            # å°‡ [1], [2], [3] ç­‰è½‰æ›ç‚º \[1\], \[2\], \[3\] ä»¥é¿å…è¢«èª¤èªç‚ºåˆ—è¡¨
            return re.sub(r'\[(\d+)\]', r'\\[\1\\]', text)

        escaped_final_report = escape_references(final_report)

        # è¨˜éŒ„æœ€çµ‚æŸ¥æ ¸å ±å‘Š - ä½¿ç”¨è½‰ç¾©å¾Œçš„å…§å®¹ä¿æŒ Markdown æ ¼å¼
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"**æœ€çµ‚æŸ¥æ ¸å ±å‘Šï¼š**\n\n{escaped_final_report}"
        })

        with st.chat_message("assistant"):
            st.markdown(f"æŸ¥æ ¸å®Œç•¢ï¼ç¸½å…±é€²è¡Œäº† {st.session_state.round_num-1} è¼ªäº’å‹•ã€‚")

        # è¨˜éŒ„æµç¨‹å®Œæˆè¨Šæ¯
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"æŸ¥æ ¸å®Œç•¢ï¼ç¸½å…±é€²è¡Œäº† {st.session_state.round_num-1} è¼ªäº’å‹•ã€‚"
        })

        st.session_state.fact_check_state = "completed"


def init_session_state():
    """åˆå§‹åŒ– session state"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "fact_check_state" not in st.session_state:
        st.session_state.fact_check_state = None
    if "current_draft" not in st.session_state:
        st.session_state.current_draft = None
    if "check_points" not in st.session_state:
        st.session_state.check_points = None
    if "resources" not in st.session_state:
        st.session_state.resources = None
    if "user_input" not in st.session_state:
        st.session_state.user_input = None
    if "round_num" not in st.session_state:
        st.session_state.round_num = 1
    if "history" not in st.session_state:
        st.session_state.history = []
    if "ai_suggested_question" not in st.session_state:
        st.session_state.ai_suggested_question = None

def display_chat_message(role: str, content: str):
    """é¡¯ç¤ºèŠå¤©è¨Šæ¯"""
    with st.chat_message(role):
        st.markdown(content)


def main():
    """ä¸»æ‡‰ç”¨ä»‹é¢"""
    st.set_page_config(
        page_title="AskCNA - äº‹å¯¦æŸ¥æ ¸åŠ©æ‰‹",
        page_icon="ğŸ”",
        layout="wide"
    )

    st.title("ğŸ” AskCNA - äº‹å¯¦æŸ¥æ ¸åŠ©æ‰‹")

    init_session_state()
    bot = StreamlitFactCheckBot()

    # å´é‚Šæ¬„
    with st.sidebar:
        st.header("ğŸ“œ ä½¿ç”¨èªªæ˜")
        st.markdown("""
        **ä½¿ç”¨æ­¥é©Ÿ:**
        1. è¼¸å…¥è¦æŸ¥æ ¸çš„æ–°èæˆ–æ¶ˆæ¯
        2. ç­‰å¾…AIåˆ†ææŸ¥æ ¸é»å’Œç”Ÿæˆè§£é‡‹
        3. æ ¹æ“šAIè©•ä¼°é¸æ“‡ä¸‹ä¸€æ­¥å‹•ä½œ
        4. æœ€çµ‚ç”Ÿæˆå®Œæ•´çš„æŸ¥æ ¸å ±å‘Š

        **é¸æ“‡é …ç›®:**
        1. æ¡ç”¨AIå»ºè­°çš„å•é¡Œ
        2. è‡ªè¡Œè¼¸å…¥å•é¡Œ
        3. ç”Ÿæˆæœ€çµ‚å ±å‘Š
        """)

        def clear_and_log():
            print("====> æ¸…é™¤å°è©±ç´€éŒ„")
            bot.reset_fact_check_state()
        st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±è¨˜éŒ„", on_click=clear_and_log)

    # é¡¯ç¤ºå°è©±æ­·å²
    for message in st.session_state.messages:
        display_chat_message(message["role"], message["content"])

    # æ ¹æ“šç‹€æ…‹é¡¯ç¤ºä¸åŒUI
    if st.session_state.fact_check_state == "waiting_user_choice":
        # ç­‰å¾…ç”¨æˆ¶é¸æ“‡ï¼Œä½¿ç”¨æŒ‰éˆ•
        # ä½¿ç”¨ session_state ä¾†è·Ÿè¸ªæŒ‰éˆ•é»æ“Šï¼Œé¿å…ç«‹å³rerun
        if "button_clicked" not in st.session_state:
            st.session_state.button_clicked = None

        if st.button("1. æ¡ç”¨AIå»ºè­°çš„å•é¡Œ", key="use_ai_btn"):
            st.session_state.button_clicked = "1"

        if st.button("2. è‡ªè¡Œè¼¸å…¥å•é¡Œ", key="custom_btn"):
            st.session_state.button_clicked = "2"

        if st.button("3. ä¸å†æå•ï¼Œç”Ÿæˆæœ€çµ‚çµæœ", key="final_btn"):
            st.session_state.button_clicked = "3"

        # è™•ç†æŒ‰éˆ•é»æ“Š
        if st.session_state.button_clicked:
            choice = st.session_state.button_clicked
            st.session_state.button_clicked = None  # é‡ç½®
            try:
                bot.handle_user_choice(choice)
                st.rerun()
            except Exception as e:
                st.error(f"âŒ è™•ç†é¸æ“‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    elif st.session_state.fact_check_state == "waiting_custom_question":
        # ç­‰å¾…ç”¨æˆ¶è¼¸å…¥è‡ªå®šç¾©å•é¡Œ
        if custom_question := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
            with st.chat_message("user"):
                st.markdown(custom_question)
            st.session_state.messages.append({"role": "user", "content": custom_question})

            try:
                bot.continue_with_question(custom_question, "ç”¨æˆ¶è¼¸å…¥")
                st.rerun()
            except Exception as e:
                st.error(f"âŒ è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    elif st.session_state.fact_check_state == "completed":
        pass

    # èŠå¤©è¼¸å…¥ - åªåœ¨æ²’æœ‰é€²è¡Œä¸­çš„æŸ¥æ ¸æ™‚é¡¯ç¤º
    elif st.session_state.fact_check_state is None:
        if prompt := st.chat_input("è«‹è¼¸å…¥è¦æŸ¥æ ¸çš„æ–°èæˆ–æ¶ˆæ¯..."):
            with st.chat_message("user"):
                st.markdown(prompt)
            st.session_state.messages.append({"role": "user", "content": prompt})

            try:
                bot.start_fact_check(prompt)
                st.rerun()
            except Exception as e:
                with st.chat_message("assistant"):
                    st.error(f"âŒ æŸ¥æ ¸éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": f"âŒ æŠ±æ­‰ï¼ŒæŸ¥æ ¸éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
                })

if __name__ == "__main__":
    main()